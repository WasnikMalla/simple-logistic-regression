{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 8 Assignment Phase 3 : Gradient Boosted Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". Do the followings in HDFS <br>\n",
    ". Remove any folder/files in /tmp that starts with flightData_ <br>\n",
    ". Put the parquet dataset file into /tmp/flightData_in/ <br>\n",
    ". Make sure put was successful (it should have the same size as the lcoal file)! <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/06/08 00:24:34 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "\n",
      "\n",
      "Deleted hdfs://localhost:9000/tmp/flightData_in\n",
      "\n",
      "\n",
      "Found 1 items\n",
      "\n",
      "\n",
      "drwxrwxrwx   - root root          0 2020-05-25 09:05 hdfs://localhost:9000/tmp/flightData_in/flightDelay.parquet\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -chmod -R 777 hdfs://localhost:9000/tmp\n",
    "! hadoop fs -rm    -r  hdfs://localhost:9000/tmp/flightData_*\n",
    "! hadoop fs -mkdir -p  hdfs://localhost:9000/tmp/flightData_in\n",
    "! hadoop fs -put   -p  flightDelay.parquet hdfs://localhost:9000/tmp/flightData_in\n",
    "! hadoop fs -ls        hdfs://localhost:9000/tmp/flightData_in/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://localhost:9000\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hdfs getconf -confKey fs.defaultFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://0bff96fd5908:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1591575903190)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-08 00:25:00,975 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@2c75dfb3\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, VectorIndexer, OneHotEncoder, PCA}\n",
       "import org.apache.spark.mllib.feature.{StandardScaler, StandardScalerModel}\n",
       "import org.apache.spark.mllib.util.MLUtils\n",
       "import org.apache.spark.ml.feature.StandardScaler\n",
       "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
       "import org.apache.spark.ml.{Pipeline, PipelineStage, PipelineModel}\n",
       "import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel, RandomForestClassifier, GBTClassifier, DecisionTreeClassifier, DecisionTreeClassificationModel}\n",
       "import org.apache.spark.ml.param.ParamMap\n",
       "import org.apache.s..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Start a simple Spark Session\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "//Feature Processing Classes\n",
    "import org.apache.spark.ml.feature.{VectorAssembler,StringIndexer,VectorIndexer,OneHotEncoder, PCA}\n",
    "\n",
    "//standard scaler Classes\n",
    "import org.apache.spark.mllib.feature.{StandardScaler, StandardScalerModel}\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "\n",
    "//Linear Algebra Data Structures\n",
    "import org.apache.spark.ml.linalg.{Vector,Vectors}\n",
    "\n",
    "//Model Building Pipeline\n",
    "import org.apache.spark.ml.{Pipeline, PipelineStage, PipelineModel}\n",
    "\n",
    "//Binary Classification\n",
    "import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel,\n",
    "                                           RandomForestClassifier, GBTClassifier,\n",
    "                                           DecisionTreeClassifier, DecisionTreeClassificationModel}\n",
    "//Model Training\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, \n",
    "                                   ParamGridBuilder, TrainValidationSplit}\n",
    "\n",
    "//Model Evaluation\n",
    "import org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator,MulticlassClassificationEvaluator}\n",
    "\n",
    "//Optional: Use the following code below to set the Error reporting\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "\n",
    "\n",
    "// GBT libraries\n",
    "\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassificationModel\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
    "\n",
    "\n",
    "val spark = SparkSession.builder().appName(\"Group 8 ML Phase 3\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in a parquet file of flight delay, fuel-price and meteorological data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading parque files and tailored dataframes as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Departing_Port: string (nullable = true)\n",
      " |-- Arriving_Port: string (nullable = true)\n",
      " |-- Airline: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month_Num: string (nullable = true)\n",
      " |-- Fuel_Price: double (nullable = true)\n",
      " |-- Departing_Port_station_ID: string (nullable = true)\n",
      " |-- Departing_Port_station_name: string (nullable = true)\n",
      " |-- Arriving_Port_station_ID: string (nullable = true)\n",
      " |-- Arriving_Port_station_name: string (nullable = true)\n",
      " |-- Mean_3pm_cloud_cover_oktas_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_dew_point_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_relative_humidity_%_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_wet_bulb_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_wind_speed_km/h_Depart: double (nullable = true)\n",
      " |-- Mean_9am_cloud_cover_okas_Depart: double (nullable = true)\n",
      " |-- Mean_9am_dew_point_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_9am_relative_humidity_%_Depart: double (nullable = true)\n",
      " |-- Mean_9am_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_9am_wet_bulb_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_9am_wind_speed_km/h_Depart: double (nullable = true)\n",
      " |-- Mean_daily_evaporation_mm_Depart: double (nullable = true)\n",
      " |-- Mean_daily_ground_minimum_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_daily_solar_exposure_MJ/m*m_Depart: double (nullable = true)\n",
      " |-- Mean_daily_sunshine_hours_Depart: double (nullable = true)\n",
      " |-- Mean_daily_wind_run_km_Depart: double (nullable = true)\n",
      " |-- Mean_maximum_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_minimum_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_clear_days_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_cloudy_days_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_0_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_2_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_30_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_35_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_40_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_1_mm_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_10_mm_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_25_mm_Depart: double (nullable = true)\n",
      " |-- Mean_rainfall_mm_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_cloud_cover_oktas_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_dew_point_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_relative_humidity_%_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_wet_bulb_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_wind_speed_km/h_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_cloud_cover_okas_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_dew_point_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_relative_humidity_%_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_wet_bulb_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_wind_speed_km/h_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_evaporation_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_ground_minimum_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_solar_exposure_MJ/m*m_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_sunshine_hours_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_wind_run_km_Arrive: double (nullable = true)\n",
      " |-- Mean_maximum_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_minimum_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_clear_days_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_cloudy_days_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_0_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_2_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_30_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_35_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_40_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_1_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_10_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_25_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_rainfall_mm_Arrive: double (nullable = true)\n",
      " |-- Date_Num: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sampleFraction: Double = 0.2\n",
       "flights: org.apache.spark.sql.DataFrame = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Take a random sample (without replacement) of the data (to reduce memory requirements)\n",
    "val sampleFraction = 0.2\n",
    "\n",
    "val flights = (spark\n",
    "            .read.parquet(\"flightDelay.parquet\")\n",
    "            .withColumn(\"Month_Num1\", $\"Month_Num\" cast \"Int\")\n",
    "            //convert month and year to integer index starting Jan 2004\n",
    "            .withColumn(\"Date_Num\",  ($\"Year\"-2004)*12 + $\"Month_Num1\")\n",
    "            .drop(\"Sectors_Flown\", \"Month_Num1\", \"Change\")\n",
    "            .withColumnRenamed(\"Departures_Delayed\",\"label\")\n",
    "            .withColumnRenamed(\"Price\",\"Fuel_Price\")\n",
    "            //take a sample without replacement\n",
    "            .sample(false,sampleFraction, seed = 222)\n",
    "            //drop NA's even though none were found!\n",
    "            .na.drop()\n",
    "            //.cache\n",
    "              )\n",
    "\n",
    "flights.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating training and test set with down sample the Ontime Departures To Balance The Training and Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balancing test set and sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set of the Most Recent 12 Months has 87119 records\n",
      "On time Test Flights: 67949\n",
      "Delayed Test FLights: 19170\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "testing: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "ontimeTestFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "delayedTestFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "sampledOntimeTestFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "sampleFraction: Double = 0.1\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Filter out the most recent 12 months of flight data as the test dataset\n",
    "//Dates after March 2019 have Date_Num > 183\n",
    "val testing = flights.filter($\"Date_Num\"> 183)\n",
    "println(s\"Test Set of the Most Recent 12 Months has ${testing.count()} records\")\n",
    "\n",
    "val ontimeTestFlights = testing.filter($\"label\" === 0)\n",
    "println(s\"On time Test Flights: ${ontimeTestFlights.count()}\")\n",
    "\n",
    "val delayedTestFlights = testing.filter($\"label\" === 1)\n",
    "println(s\"Delayed Test FLights: ${delayedTestFlights.count()}\")\n",
    "\n",
    "val sampledOntimeTestFlights = ontimeTestFlights.sample(false, 0.2)\n",
    "\n",
    "val sampleFraction = 0.1\n",
    "\n",
    "val test = (sampledOntimeTestFlights\n",
    "                .union(delayedTestFlights)\n",
    "                .sample(false, sampleFraction))\n",
    "\n",
    "//val sampledCounts = test.groupBy(\"label\").count()\n",
    "//println(\"proportion of lates (label=1) in the sample\")\n",
    "//sampledCounts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balancing training set and sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On time Training Flights: 977102\n",
      "Delayed Training Flights: 193922\n",
      "Down Sampled ontime Training Flights: 195075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rawTraining: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "ontimeTrainingFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "delayedTrainingFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "sampledOntimeTrainingFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "sampleFraction: Double = 0.1\n",
       "train: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Filter out rows prior to the most recent 12 months of flight data as the training dataset\n",
    "val rawTraining = flights.filter($\"Date_Num\" < 184)\n",
    "\n",
    "val ontimeTrainingFlights = rawTraining.filter($\"label\"===0)\n",
    "println(s\"On time Training Flights: ${ontimeTrainingFlights.count()}\")\n",
    "\n",
    "val delayedTrainingFlights = rawTraining.filter($\"label\"===1)\n",
    "println(s\"Delayed Training Flights: ${delayedTrainingFlights.count()}\")\n",
    "\n",
    "val sampledOntimeTrainingFlights = ontimeTrainingFlights.sample(false, 0.2)  \n",
    "\n",
    "println(s\"Down Sampled ontime Training Flights: ${sampledOntimeTrainingFlights.count()}\")\n",
    "\n",
    "val sampleFraction = 0.1\n",
    "//Concatenate rows of ontimeTrainingFlights and delayedTrainingFlights\n",
    "val train = (sampledOntimeTrainingFlights\n",
    "                .union(delayedTrainingFlights)\n",
    "                .sample(false, sampleFraction))\n",
    "               \n",
    "//val resampledCounts = training.groupBy(\"label\").count()\n",
    "//println(\"proportion of lates (label=1) in the sample\")\n",
    "//resampledCounts.show()\n",
    "//training.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getConfusionMatrix: (predictionDF: org.apache.spark.sql.DataFrame)Unit\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def getConfusionMatrix(predictionDF: DataFrame): Unit = {\n",
    "    \n",
    "    println(\"========================Model Assessment Metrics==================================================\\n\")\n",
    "    // Define Binary Classification Evaluator\n",
    "    val binaryEval = new BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"rawPrediction\")\n",
    "    // Run Evaluation.  The area under the ROC curve ranges from 0.5 and 1.0 with larger values indicative of better fit\n",
    "    println(s\"Area under ROC: ${binaryEval.setMetricName(\"areaUnderROC\").evaluate(predictionDF)}\")\n",
    "    // Define Multiclass Classification Evaluator\n",
    "    val multiEval = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\")\n",
    "    println(s\"Accuracy: ${multiEval.setMetricName(\"accuracy\").evaluate(predictionDF)}\")\n",
    "    println(s\"Weighted Precision: ${multiEval.setMetricName(\"weightedPrecision\").evaluate(predictionDF)}\")\n",
    "    println(s\"Weighted Recall: ${multiEval.setMetricName(\"weightedRecall\").evaluate(predictionDF)}\")\n",
    "    println(s\"F1: ${multiEval.setMetricName(\"f1\").evaluate(predictionDF)}\")\n",
    "\n",
    "    val TP = predictionDF.select(\"label\", \"prediction\").filter(\"label = 1 and prediction = 1\").count\n",
    "    val TN = predictionDF.select(\"label\", \"prediction\").filter(\"label = 0 and prediction = 0\").count\n",
    "    val FP = predictionDF.select(\"label\", \"prediction\").filter(\"label = 0 and prediction = 1\").count\n",
    "    val FN = predictionDF.select(\"label\", \"prediction\").filter(\"label = 1 and prediction = 0\").count\n",
    "    val total = predictionDF.select(\"label\").count.toDouble\n",
    "    // Unweighted Metrics\n",
    "    val accuracy    = (TP + TN) / total\n",
    "    val precision   = TP / (TP+FP).toDouble\n",
    "    val recall      = TP / (TP+FN).toDouble\n",
    "    val F1 = 2*precision*recall/(precision+recall)\n",
    "    println(s\"Accuracy: ${accuracy}\")\n",
    "    println(s\"Precision: ${precision}\")\n",
    "    println(s\"Recall: ${recall}\")\n",
    "    println(s\"F1: ${F1}\")\n",
    "\n",
    "    //predictionDF.select( $\"label\",$\"prediction\" cast \"Int\").orderBy(\"label\").groupBy(\"label\").pivot(\"prediction\",Seq(\"0\",\"1\")).count.show()\n",
    "\n",
    "    // Confusion matrix\n",
    "    printf(s\"\"\"|=================== Confusion Matrix ==========================\n",
    "           |##########| %-15s                     %-15s\n",
    "           |----------+----------------------------------------------------\n",
    "           |Actual = 0| %-15d                     %-15d\n",
    "           |Actual = 1| %-15d                     %-15d\n",
    "           |===============================================================\n",
    "         \"\"\".stripMargin, \"Predicted = 0\", \"Predicted = 1\", TN, FP, FN, TP)\n",
    "\n",
    "    println(\"\\n==================================================================================================\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preparing feature vectors for ML pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre processing continuos features : Extracting Weather columns along with Date_Num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|    weather_features|\n",
      "+--------------------+\n",
      "|[3.9,7.4,43.0,21....|\n",
      "|[5.8,23.1,69.0,29...|\n",
      "|[5.8,23.1,69.0,29...|\n",
      "|[5.8,23.1,69.0,29...|\n",
      "|[3.9,7.4,43.0,21....|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array: Array[String] = Array(Departing_Port, Arriving_Port, Airline, label, Year, Month_Num, Fuel_Price, Departing_Port_station_ID, Departing_Port_station_name, Arriving_Port_station_ID, Arriving_Port_station_name)\n",
       "columns: Array[String] = Array(Departing_Port, Arriving_Port, Airline, label, Year, Month_Num, Fuel_Price, Departing_Port_station_ID, Departing_Port_station_name, Arriving_Port_station_ID, Arriving_Port_station_name, Mean_3pm_cloud_cover_oktas_Depart, Mean_3pm_dew_point_temperature_Degrees_C_Depart, Mean_3pm_relative_humidity_%_Depart, Mean_3pm_temperature_Degrees_C_Depart, Mean_3pm_wet_bulb_temperature_Degrees_C_Depart, Mean_3pm_wind_speed_km/h_Depart, Mean_9am_cloud_cover_okas_Depart, Mean_9am_dew_point_temperature_Degrees_C_Depart, Mean_9am_relative_humidity_%_Depart, Mean..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// listing columns and extracting as requried\n",
    "val array = Array(\"Departing_Port\", \"Arriving_Port\", \"Airline\", \"label\", \"Year\", \"Month_Num\", \"Fuel_Price\", \n",
    "                  \"Departing_Port_station_ID\", \"Departing_Port_station_name\", \"Arriving_Port_station_ID\", \n",
    "                  \"Arriving_Port_station_name\")\n",
    "val columns = flights.columns\n",
    "\n",
    "val feature_cols = columns.filter(!array.contains(_))\n",
    "\n",
    "\n",
    "// Using VectorAssembler to create single vector feature out of feature_cols\n",
    "\n",
    "val weather_assembler = new VectorAssembler()\n",
    "                        .setInputCols(feature_cols)\n",
    "                        .setOutputCol(\"weather_features\")\n",
    "\n",
    "val weathered_DF = weather_assembler.transform(flights)\n",
    "\n",
    "//weathered_DF.printSchema\n",
    "val weather_features_df = weathered_DF.select(\"weather_features\")\n",
    "weather_features_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardridzing weather_features_df using StandardScalar such that output feature can be used for PCA to reduce dimensionality of weather features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StandardScaler\n",
       "std_scalar: org.apache.spark.ml.feature.StandardScaler = stdScal_23687a218e2c\n",
       "pca: org.apache.spark.ml.feature.PCA = pca_32e00ee50e04\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "// creating standardarized features\n",
    "val std_scalar = new StandardScaler() \n",
    "                .setInputCol(\"weather_features\")\n",
    "                .setOutputCol(\"standard_features\")\n",
    "                .setWithStd(true)\n",
    "                .setWithMean(false)\n",
    "\n",
    "// implementing to PCA\n",
    "val pca = new PCA()\n",
    "          .setInputCol(\"standard_features\")\n",
    "          .setOutputCol(\"continuos_features\")\n",
    "          .setK(14) // k= 2 becasue for depart and arrive weather data along with Date_Num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-processing Categorical features : usign VectorAssembler to create a single vector feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categoricalVariables: Array[String] = Array(Departing_Port, Arriving_Port, Airline)\n",
       "categoricalIndexers: Array[org.apache.spark.ml.feature.StringIndexer] = Array(strIdx_fd01ff548dcf, strIdx_39f1bba81815, strIdx_bcc8319db319)\n",
       "categoricalEncoders: Array[org.apache.spark.ml.feature.OneHotEncoder] = Array(oneHot_118c3197756e, oneHot_694627f5a950, oneHot_41ef0fd6e9fa)\n",
       "explanatoryFields: Array[String] = Array(Airline_Vec, Fuel_Price, Departing_Port_Vec, Arriving_Port_Vec)\n",
       "categorical_assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_ba2306d4dd29\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Deal with Categorical Columns\n",
    "val categoricalVariables = Array(\n",
    "    \"Departing_Port\", \"Arriving_Port\", \"Airline\")\n",
    "val categoricalIndexers = categoricalVariables\n",
    "  .map(i => new StringIndexer().setInputCol(i).setOutputCol(i+\"_Index\"))\n",
    "val categoricalEncoders = categoricalVariables\n",
    "  .map(e => new OneHotEncoder().setInputCol(e + \"_Index\").setOutputCol(e + \"_Vec\"))\n",
    "\n",
    "\n",
    "// selecting only vectorized explanatory categorical fields\n",
    "val explanatoryFields = Array(\"Airline_Vec\", \"Fuel_Price\",\"Departing_Port_Vec\",\"Arriving_Port_Vec\")\n",
    "\n",
    "// Assemble everything together to be (\"label\",\"features\") format\n",
    "val categorical_assembler = new VectorAssembler()\n",
    "                 .setInputCols(explanatoryFields)\n",
    "                 //.setOutputCol(\"indexedFeatures\")\n",
    "                 .setOutputCol(\"categorical_features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using VectorAssembler again to finally bind categorical and continuous features as features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_ce997d24cd79\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// creating final features\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "                .setInputCols(Array(\"continuos_features\", \"categorical_features\"))\n",
    "                .setOutputCol(\"features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building stages for Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featureProcessingStages: Array[org.apache.spark.ml.PipelineStage] = Array(vecAssembler_cf635f19a171, stdScal_23687a218e2c, pca_32e00ee50e04, strIdx_fd01ff548dcf, strIdx_39f1bba81815, strIdx_bcc8319db319, oneHot_118c3197756e, oneHot_694627f5a950, oneHot_41ef0fd6e9fa, vecAssembler_ba2306d4dd29, vecAssembler_ce997d24cd79)\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// establishing stages of pipeling\n",
    "\n",
    "val featureProcessingStages: Array[PipelineStage] = Array(weather_assembler, std_scalar, pca) ++ categoricalIndexers ++ \n",
    "                             categoricalEncoders ++ Array(categorical_assembler) ++ Array(assembler) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.  Estimator Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gbt: org.apache.spark.ml.classification.GBTClassifier = gbtc_cd385390062e\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_5c8dbeb451ed\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// creating gradient boosted tree classifier\n",
    "val gbt = new GBTClassifier()\n",
    "         .setLabelCol(\"label\")\n",
    "         .setFeaturesCol(\"features\")\n",
    "         .setMaxIter(10)\n",
    "         .setMaxBins(10)\n",
    "         .setFeatureSubsetStrategy(\"auto\")\n",
    "\n",
    "// assembling a pipeline\n",
    "val pipeline = new Pipeline()\n",
    "              .setStages(featureProcessingStages ++ Array(gbt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Fitting model into data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-08 00:28:16,734 WARN  [Executor task launch worker for task 30] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2020-06-08 00:28:16,735 WARN  [Executor task launch worker for task 30] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "2020-06-08 00:28:34,968 WARN  [Thread-4] netlib.LAPACK (LAPACK.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "2020-06-08 00:28:34,969 WARN  [Thread-4] netlib.LAPACK (LAPACK.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.PipelineModel = pipeline_5c8dbeb451ed\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// fitting pipeline to training set\n",
    "val model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prediction: org.apache.spark.sql.DataFrame = [Departing_Port: string, Arriving_Port: string ... 86 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// transforming test into model\n",
    "val prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================Model Assessment Metrics==================================================\n",
      "\n",
      "Area under ROC: 0.592478005010779\n",
      "Accuracy: 0.575653798256538\n",
      "Weighted Precision: 0.5900731776867956\n",
      "Weighted Recall: 0.575653798256538\n",
      "F1: 0.5793289302851267\n",
      "Accuracy: 0.575653798256538\n",
      "Precision: 0.6644498186215235\n",
      "Recall: 0.5762978500262191\n",
      "F1: 0.617242347655153\n",
      "=================== Confusion Matrix ==========================\n",
      "##########| Predicted = 0                       Predicted = 1  \n",
      "----------+----------------------------------------------------\n",
      "Actual = 0| 750                                 555            \n",
      "Actual = 1| 808                                 1099           \n",
      "===============================================================\n",
      "         \n",
      "==================================================================================================\n"
     ]
    }
   ],
   "source": [
    "// calling a function for model assessment\n",
    "\n",
    "getConfusionMatrix(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
